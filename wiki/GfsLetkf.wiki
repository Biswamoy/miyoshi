#summary Documentation for GFS-LETKF

<wiki:toc max_depth="3" />

= Getting started =

== Download our LETKF package ==

Download the latest release version from the [http://code.google.com/p/miyoshi/source/checkout Source tab].

== Prepare required executables and libraries ==

Our LETKF package provides the LETKF assimilation code coupled to the National Centers for Environmental Prediction (NCEP) Global Forecasting System (GFS) model. It requires several executables and libraries from the NCEP, which mainly belong to two NCEP models: GFS and Gridpoint Statistical Interpolation (GSI). Compiling these libraries and models on your computer may not be easy and may take long time.

 * *GFS*: The GFS model is an operational global NWP model developed by the Environmental Modeling Center (EMC) at the NCEP. It is one of the major state-of-the-art operational NWP models over the world and provides main model guidance for the weather forecast in the United States. It is not a community model, and some of the code may not be open to public, so users interested to run the GFS-LETKF system need to contact NCEP to obtain the GFS model code and the associated libraries.
  * NCEP/EMC GFS model page: [http://www.emc.ncep.noaa.gov/index.php?branch=GFS]
  * When you compile the GFS model, you may need to compile the [http://www.earthsystemmodeling.org/ Earth System Modeling Framework (ESMF)] first.

 * *GSI*: The GSI is the primary data assimilation system for the GFS model, based on 3-dimensional variational method (3DVar). In our GFS-LETKF implementation, there is an option to use the GSI as an observation operator. Note that we only use the GSI as the observation operator, and the GSI 3DVar solution is not computed in the GFS-LETKF. Besides, if you do not plan to assimilate the satellite radiance data, in the GFS-LETKF package there is also a set of built-in observation operators that does not rely on the GSI. In this case, you do not need to install the GSI.
  * The GSI has been a community model, available at [http://www.dtcenter.org/com-GSI/users/]

=== List of required executables and libraries ===

 * *NCEP executables*:
 After successfully compiling the GFS and GSI, you are asked to put all executable files together in the '{{{$EXECGLOBAL}}}' directory configured in '{{{gfs/run/configure.sh}}}'.
|| *Name* || *Purpose* || *Called from* ||
|| {{{global_fcst}}} || GFS main program || {{{gfs/run/run_gfs.sh}}}<br>{{{gfs/run/cycle.sh}}}<br>{{{gfs/run/fcst.sh}}} ||
|| {{{global_gsi}}} || GSI main program || {{{gfs/run/run_gsi.sh}}}<br>{{{gfs/run/cycle.sh}}} ||
|| {{{global_sighdr}}} || Read the header of the GFS sigma-level input/output files || {{{gfs/run/run_gfs.sh}}}<br>{{{gfs/run/run_gsi.sh}}} ||
|| {{{global_sfchdr}}} || Read the header of the GFS surface input/output files || {{{gfs/run/run_gfs.sh}}}<br>{{{gfs/run/run_gsi.sh}}} ||
|| {{{global_chgres}}} || Change the resolution of the GFS input/output files || {{{gfs/run/run_chgres.sh}}} ||

 * *NCEP libraries required for compiling the GFS-LETKF code*:
 These libraries are required for compiling several programs in the GFS-LETKF package. The path of these libraries are configured in '{{{gfs/configure.user}}}' :
|| *Name* || *Purpose* || *List of files required* || *Link* ||
|| {{{bacio}}} || Basic I/O library for the GFS || {{{$(BACIO_LIB)/libbacio_4.a}}} ||  ||
|| {{{sigio}}} || I/O library for the GFS sigma-level files || {{{$(SIGIO_LIB)/libsigio_4.a}}}<br>{{{$(SIGIO_INC)/sigio_module.mod}}}<br>{{{$(SIGIO_INC)/sigio_r_module.mod}}} ||  ||
|| {{{sfcio}}} || I/O library for the GFS surface-level files || {{{$(SFCIO_LIB)/libsfcio_4.a}}}<br>{{{$(SFCIO_INC)/sfcio_module.mod}}} ||  ||
|| {{{sp}}} || Spectral transform library || {{{$(SP_LIB)/libsp_4.a}}} || [http://www.nco.ncep.noaa.gov/pmb/docs/libs/splib/ncep_splib.shtml] ||
|| {{{bufrlib}}} || I/O library for the BUFR file format || {{{$(BUFR_LIB)/libbufrlib.a}}} || [http://www.nco.ncep.noaa.gov/sib/decoders/BUFRLIB/] ||

 * *Other NCEP libraries that may be required for compiling the GFS and GSI*:
 These libraries may be required when you compile the GFS and GSI, but they are NOT directly required in the GFS-LETKF.
|| *Name* || *Purpose* ||
|| {{{ip}}} || General interpolation library ||
|| {{{landsfcutil}}} || Land surface library ||
|| {{{gfsio}}} || GFS I/O library ||
|| {{{nemsio}}} || NEMS I/O library ||
|| {{{w3lib}}} || I/O library for the GRIB file format ||

 * *Other required libraries*:
 LAPACK may be required when it is not provided by the Fortran compiler. For example, if you use the Intel Fortran compiler with its Math Kernel Library (MKL), the LAPACK is included and you do not need to install LAPACK by yourself.
|| *Name* || *Purpose* || *Link* ||
|| {{{LAPACK}}} || Linear algebra package || [http://www.netlib.org/lapack/] ||

== Prepare fix files for the GFS and GSI ==

To run the GFS and GSI, they require several "fix" data files (e.g., boundary conditions) and CRTM coefficients. These files need to be put in three directories: '{{{$FIXGLOBAL}}}', '{{{$FIXGSI}}}', and '{{{$FIXCRTM}}}', respectively, configured in '{{{gfs/run/configure.sh}}}'.

The GFS fix files can be found at [http://www.nco.ncep.noaa.gov/pmb/codes/nwprod/fix/] , but they are much more than enough to drive a GFS model. It is recommended to select only the necessary files when putting into '{{{$FIXGLOBAL}}}'.

== Test compiling the GFS-LETKF programs ==

When the required libraries are ready, we can do a test compilation of all GFS-LETKF programs.

 * Go to the GFS main directory.
{{{
$ cd gfs
}}}
 * Edit the '{{{configure.user}}}' file, specify the compiler flags and paths of the required libraries in your system. In the '{{{arch}}}' directory, there are a few examples of the '{{{configure.user}}}' file using the PGI or Intel compilers.
 * Compile all GFS-LETKF programs.
{{{
$ make
}}}

=== List of the GFS-LETKF programs ==

If the compilation is successful, you will obtain a number of executable files listed below:
|| *File path and name* || *Purpose* ||
|| {{{common/datetime}}} || A utility for date and time computation ||
|| {{{common/enssize}}} || To simply print the ensemble size ||
|| {{{letkf/efsoXXX}}} || EFSO mean program ||
|| {{{letkf/letkfXXX}}} || LETKF main program ||
|| {{{letkf/meanXXX}}} || To compute the ensemble mean from grid files ||
|| {{{letkf/obsope}}} || Built-in observation operator for non-radiance data ||
|| {{{obs/dec_prcp}}} || To convert the gridded precipitation data to the LETKF observation format ||
|| {{{obs/dec_prepbufr}}} || To convert the NCEP PREPBUFR data to the LETKF observation format ||
|| {{{readdiag_conv}}} || To convert the GSI diagnostic files to the LETKF observation format with model background values ||
|| {{{obs/superob}}} || A superobing/thinning utility ||
|| {{{ssio/grd2ss}}} || To convert a grid file to GFS sig/sfc (spectral) files, and also cycle forecasts into analyses ||
|| {{{ssio/grdctl}}} || To print GrADS CTL files according to the current grid settings ||
|| {{{ssio/ss2grd}}} || To convert GFS sig/sfc (spectral) files to a grid file ||
|| {{{ssio/ss2grdp}}} || To convert GFS sig/sfc (spectral) files to a grid file in pressure coordinates ||
|| {{{ssio/sscycle}}} || To cycle forecasts into analyses ||
|| {{{util/gfsmeanXXX}}} || To compute the ensemble mean from GFS sig/sfc (spectral) files ||
|| {{{verify/verify}}} || A utility to perform verification against observations or other model analyses ||

Note that {{{XXX}}} is the ensemble size that the program is used for.

If some executable files are missing, check the errors and try to fix them.

= Code overview =

== List of scripts ==
All run scripts are located in the '{{{gfs/run}}}' directory, so you will do every task in this directory. The purposes of the scripts are described as follows. Note that for most of the scripts, you can show the usage help by directly executing them with no arguments.

|| *Script* || *Purpose* ||
|| {{{configure.sh}}} || Main configurations for all GFS-LETKF scripts. ||
|| {{{datetime.sh}}} || Date and time functions. It uses the Unix 'date' command.<br>If the 'date' command does not work in your system, replace '{{{datetime.sh}}}' by '{{{datetime.sh.no_unix_date}}}' that does not require the 'date' command. ||
|| {{{distribution.sh}}} || Functions to adaptively distribute members on nodes. ||
|| {{{stageinout.sh}}} || Functions to copy files between the server node and local disks on computing nodes. ||
|| {{{get_ncepobs.sh}}} || Download NCEP conventional observation data from the CISL Research Data Archive. ||
|| {{{get_cfsr.sh}}} || Download NCEP CFSR data. ||
|| {{{run_chgres.sh}}} || Change the resolution of a series of GFS initial conditions. ||
|| {{{mss2grd.sh}}} || Convert GFS sig/sfc files to GrADS grd/grdp files. ||
|| {{{outdir.sh}}} || Create necessary subdirectories and files in the output ({{{$OUTDIR}}}) directory. ||
|| {{{init.sh}}} || Prepare a initial ensemble from a series of analyses at different times. ||
|| {{{init2.sh}}} || Prepare a initial ensemble from outputs of another experiments. ||
|| {{{init3.sh}}} || Prepare a series of initial mean analyses from another source, which is useful to run forecast experiments. ||
|| {{{run_gfs.sh}}} || Prepare a temporary directory for a GFS run, and may run the model. ||
|| {{{run_gsi.sh}}} || Prepare a temporary directory for a GSI run, and may run the model. ||
|| {{{cycle.sh}}} || Run a GFS-LETKF forecast/analysis cycle. ||
|| {{{mcycle.sh}}} || Run multiple cycles. ||
|| {{{fcst.sh}}} || Run (ensemble) forecasts and may also perform forecast verification. ||
|| {{{mfcst.sh}}} || Run multiple-cycle ensemble mean forecasts. ||
|| {{{efsofcst.sh}}} || Run multiple-cycle ensemble forecasts for the EFSO computation. ||
|| {{{verify.sh}}} || Compute verification of (ensemble) forecasts. ||
|| {{{mverify.sh}}} || Run multiple-cycle verification. ||
|| {{{efso.sh}}} || Compute the ensemble forecast sensitivity to observations (EFSO). ||
|| {{{mefso.sh}}} || Run multiple-cycle EFSO. ||
|| {{{pbs_example.sh}}} || An example script to submit a parallel job with the PBS jobs scheduling system. ||

== Main configuration script ==

The '{{{configure.sh}}}' is the main configuration file. The other scripts are run based on this file. The explanation of all the configurable variables are provided along with the file (using code comments). Selected variables are explained here:

|| *Variable* || *Description* ||
|| {{{OUTDIR}}} || GFS-LETKF input and output directory. ||
|| {{{SYSNAME}}} || A unique name in the machine, which is used to identify multiple GFS-LETKF runs. ||
|| {{{LTMP1}}}<br>{{{LTMP2}}} || Level 1 and 2 local temporary directories on computing nodes. They are only used when {{{$SHAREDISK = 0}}}. They must exist on all computing nodes. The capacity requirement of the level 1 temporary directory is not too big but the I/O speed is very important; therefore, a RAMdisk ({{{/dev/shm}}} in Linux machines) could be assigned to this variable if the memory on your machine is sufficient. The capacity requirement of the level 2 temporary directory is bigger than {{{$LTMP1}}} and the I/O speed is less important than {{{$LTMP1}}}. The {{{$LTMP1}}} and the {{{$LTMP2}}} can be assigned to the same directory if you do not want to separate the location of different temporary files. ||
|| {{{TMP1}}} || Temporary directory on the server machine. Accessing to this directory from other computing nodes is not required. ||
|| {{{TMPMPI}}} || Temporary directory on the server machine with shared access from all computing nodes. ||
|| {{{SHAREDISK}}} || Do we use local disks on computing nodes to store runtime temporary file?<br>0: Yes, use local disks ({{{$LTMP1}}}, {{{$LTMP2}}}) to store temporary files.<br>1: No, the {{{$OUTDIR}}} is a shared disk that can be accessed from all computing nodes. Use this shared disk to store temporary files.||
|| {{{EXECGLOBAL}}} || Directory of the [#List_of_required_executables_and_libraries NCEP executable files] (GFS, GSI... etc.). ||
|| {{{FIXGLOBAL}}} || Directory of GFS fix files. ||
|| {{{FIXGSI}}} || Directory of GSI fix files, only required when using GSI as the observation operator ({{{$OBSOPE_OPT = 2}}}). ||
|| {{{FIXCRTM}}} || Directory of CRTM fix files, only required when using GSI as the observation operator ({{{$OBSOPE_OPT = 2}}}). ||
|| {{{ANLGFS}}} || Directory of reference model files in the GFS sig/sfc formats. ||
|| {{{ANLGRD}}} || Directory of reference model files in the sigma-level grid format (grd). ||
|| {{{ANLGRDP}}}<br>{{{ANLGRDP2}}} || Directory of reference model files in the pressure-level grid format (grdp). They are used for verification. The verification results against the model data in {{{$ANLGRDP}}} will be stored in '{{{$OUTDIR/verfa1}}}'; the verification results against the model data in {{{$ANLGRDP2}}} will be stored in '{{{$OUTDIR/verfa2}}}'. ||
|| {{{INITGFS}}} || Directory of arbitrary initial condition files in the GFS sig/sfc formats. It can be the same as {{{$ANLGFS}}} if the quality of the reference model data in that directory is good. See explanation in the [#Prepare_the_GFS_initial_conditions latter section]. ||
|| {{{OBS}}} || Directory of observation data in the LETKF observation format, required when using the built-in observation operator ({{{$OBSOPE_OPT = 1}}}). ||
|| {{{OBSNCEP}}} || Directory of observation data in the NCEP BUFR format, required when using GSI as the observation operator ({{{$OBSOPE_OPT = 2}}}). ||
|| {{{MEMBER}}} || Ensemble size. It should be the same as the '{{{nbv}}}' variable in '{{{common/common_letkf.f90}}}'. ||
|| {{{MIN_NP_GFS}}}<br>{{{MIN_NP_GSI}}} || Minimum numbers of CPU cores required to run the GFS and GSI. This limits are to avoid using up all available memory per node/core. ||
|| {{{MAX_NP_GFS}}}<br>{{{MAX_NP_GSI}}} || Maximum numbers of cores suggested to run the GFS and GSI. This limits are to avoid poor parallelization efficiency when using too many nodes/cores. ||
|| {{{OBSOPE_OPT}}} || Observation operator options:<br>1: use the LETKF built-in observation operators.<br>2: use the GSI as the observation operator. ||
|| {{{THIN_OPT}}} || Superobing/thinning options:<br>-- Options below (1-2) is for {{{$OBSOPE_OPT = 1}}}<br>1: No superobing/thinning.<br>2: Use superobed/thinned observations processed by the '{{{gfs/obs/superob}}}' program before the LETKF assimilation.<br>-- Options below (3-4) are for {{{$OBSOPE_OPT = 2}}}<br>3: use thinned observations for satellite radiance observations only, processed by the GSI.<br>4: use thinned observations for both conventional and satellite radiance observations, processed by the GSI. ||
|| {{{ADAPTINFL}}} || Adaptive inflation options:<br>0: OFF, no adaptive inflation.<br>1: ON, using inflation parameter 1 cycle ago as the prior.<br>2: ON, using inflation parameter 2 cycles ago as the prior (leap-frog the adaptive inflation fields). ||
|| {{{FCSTLEN}}} || GFS forecast length when running the (ensemble) forecasts ('{{{gfs/run/fcst.sh}}}'). ||
|| {{{OUT_OPT}}}<br>{{{FOUT_OPT}}}<br>{{{OBSOUT_OPT}}}<br>{{{LOG_OPT}}} || How detail do you want to keep for the regular and diagnostic output files in '{{{$OUTDIR}}}'? See details in the '{{{configure.sh}}}' script. ||
|| {{{MPIBIN}}} || The path of the '{{{mpiexec}}}' command to run a parallel program. ||
|| {{{BUFRBIN}}} || The path of the '{{{grabbufr}}}' command. ||

== Variables in the source code ==

There are other variables that can not be configured in the '{{{configure.sh}}}', but need to be set in the Fortran source code before compiling the GFS-LETKF programs. After changing the values of these variables, the GFS-LETKF programs need to be re-compiled. Some important variables in the source code are listed and explained below:

|| *Source file* || *Variable* || *Description* ||
|| {{{common/common_letkf.f90}}} || {{{nbv}}} || Ensemble size ||
|| {{{gfs/common/common_gfs.f90}}} || {{{nlon}}}<br>{{{nlat}}} || Longitude and latitude grid numbers of the GFS model. They correspond to the spectral truncation wavenumbers:<br>For T62, {{{$nlon}}} = 192, {{{$nlat}}} = 94<br>For T126, {{{$nlon}}} = 384, {{{$nlat}}} = 190 ||
|| {{{gfs/common/common_gfs.f90}}} || {{{nlev}}} || Number of the vertical levels of the GFS model ||
|| {{{gfs/common/common_gfs.f90}}} || {{{gfs_jcap}}} || Spectral truncation wavenumber of the GFS model ||
|| {{{gfs/common/common_gfs_pres.f90}}} || {{{nlevp}}} || Number of the vertical levels in the GFS-LETKF pressure-level outputs ||
|| {{{gfs/common/common_gfs_pres.f90}}} || {{{levp}}} || List of the vertical levels in the GFS-LETKF pressure-level outputs ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{omb_output}}} || Whether output the (observation - background) statistics? ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{oma_output}}} || Whether output the (observation - analysis) statistics? ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{obsgues_output}}} || Whether output the observation values in the ensemble model background [H(X^b^)]? ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{obsanal_output}}} || Whether output the observation values in the ensemble model analyses [H(X^a^)]? This is used for the EFSO computation, and it requires considerable additional computational time of the LETKF main program. ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{sigma_obs}}} || Horizontal localization length scale ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{sigma_obsv}}} || Vertical localization length scale ||
|| {{{gfs/letkf/letkf_obs.f90}}} || {{{gross_error}}} || Quality control with gross errors ||
|| {{{gfs/letkf/letkf_tools.f90}}} || {{{cov_infl_mul}}} || Multiplicative  covariance inflation parameter:<br>> 0: globally constant inflation<br>< 0: 3D inflation values input from the '{{{infl_mul.grd}}}' file ||
|| {{{gfs/letkf/letkf_tools.f90}}} || {{{sp_infl_mul}}} || Additive covariance inflation parameter ||
|| {{{gfs/letkf/letkf_tools.f90}}} || {{{var_local}}} || Variable localization matrix ||
|| {{{gfs/obs/superob.f90}}} || {{{obmethod_g}}}<br>{{{obmethod_v}}}<br>{{{obmethod_t}}}<br>{{{obmethod_h}}} || Superobing and thinning settings. See details in the code comments. ||
|| {{{gfs/verify/verify.f90}}} || {{{nvrf_obs}}}<br>{{{nvrf_ana}}} || Numbers of the observation datasets and the model analysis datasets used for the verification ||
|| {{{gfs/verify/verify.f90}}} || {{{narea}}}<br>{{{vlon1}}}<br>{{{vlon2}}}<br>{{{vlat1}}}<br>{{{vlat2}}} || Settings of the verification regions ||

== Data formats ==

Below are several data formats appeared in the GFS-LETKF system:

 * *GFS sigma/surface files*
 GFS model inputs and outputs.<br>Abbreviation: *sig/sfc*

 * *GrADS grid files in sigma coordinate (same as the GFS model levels)*
 Gridded files in model levels that can be read by the LETKF main program and plotted with the GrADS software.<br>Abbreviation: *grd*

 * *GrADS grid files in pressure coordinate*
 Gridded files in pressure levels, which is defined by '{{{nlevp}}}' and '{{{levp}}}' in '{{{gfs/common/common_gfs_pres.f90}}}'. It can be plotted with the GrADS software.<br>Abbreviation: *grdp*

 * *NCEP PREPBUFR files*
 NCEP PREPBUFR observation date format.<br>Abbreviation: *prepbufr*

 * *LETKF observation format*
 A special observation data format used by the LETKF code.<br>Abbreviation: *letkfobs*

 The observation data are saved in a simple binary structure. For a single observation, it consists of these columns, all of which are 4-bytes floating variables:
|| *Column* || *Description* ||
|| 1 || Variable type; see '{{{id_*_obs}}}' variables in '{{{gfs/common/common_obs_gfs.f90}}}' ||
|| 2 || Longitude (degree) ||
|| 3 || Latitude (degree) ||
|| 4 || u,v,t,tv,q,rh: level (hPa)<br>ps: station elevation (m) ||
|| 5 || Observation value:<br>wind (m/s)<br>temperature (K)<br>specific humidity (kg/kg)<br>relative humidity (%)<br>surface pressure (hPa) ||
|| 6 || Observation error (unit same as the observation value) ||
|| 7 || Observation platform type; see the '{{{obtypelist}}}' array in '{{{gfs/common/common_obs_gfs.f90}}}' ||

 * *LETKF observation format with additional columns*
 It is an extension of the "letkfobs" format with more columns to save the observation value in the model.<br>Abbreviation: *letkfobs2*

 However, the meaning of the additional columns varies in different cases. In the observation data processed by '{{{obsope}}}' or '{{{readdiag_*}}}' programs, or the input data of the LETKF main program:
|| *Column* || *Description* ||
|| 8 || Observation time relative to analysis time (hour; can be positive or negative) ||
|| 9 || h(x) observation in model background (unit same as observation value except surface pressure in Pa) ||
|| 10 || quality control mark (1 = pass; others = do not pass) ||

 In the observation diagnostics output by the LETKF program ({{{$OUTDIR/obs/obsdiag}}}):
|| *Column* || *Description* ||
|| 8 || Observation time relative to analysis time (hour; can be positive or negative) ||
|| 9 || h(x) observation in model, either background or analysis (unit same as observation value except surface pressure in Pa) ||
|| 10 || 0 (no meaning) ||

 In the observation departure statistics output by the LETKF program ({{{$OUTDIR/obs/obsdep}}}):
|| *Column* || *Description* ||
|| 8 || Observation time relative to analysis time (hour; can be positive or negative) ||
|| 9 || (Observation - background) (O-B) or (Observation - analysis) (O-A) ||
|| 10 || 0 (no meaning) ||

 In the observation sensitivity estimates output by the EFSO program ({{{$OUTDIR/efso}}}):
|| *Column* || *Description* ||
|| 8 || Observation time relative to analysis time (hour; can be positive or negative) ||
|| 9 || Observation sensitivity estimated by the EFSO (J/kg) ||
|| 10 || 0 (no meaning) ||

 * *GSI diagnostic files*
 The format of the GSI diagnostic outputs<br>Abbreviation: *gsidiag*

== Flow chart ==

[http://miyoshi.googlecode.com/svn/wiki/images/gfs-letkf-flowchart.png]

In the flow chart, the rectangles represent any kind of files with their formats shown in square brackets ([#Data_formats explanation of the data formats and their abbreviations]). Those rectangles are connected by arrow lines that represent program execution, with corresponding program file names shown in the bold italic font next to the arrow lines. Three main components of the system – the data assimilation cycle, the observation processing module, and the model forecast and verification modules – are boxed by the red dashed rectangles. The data assimilation cycle is illustrated in the lower part of the figure: the 9-hour ensemble GFS model integration is executed based on the GFS sigma/surface file formats (sig/sfc), and the LETKF analysis is executed based on the grid file format (grd). The purpose of conducting 9-hour forecasts is to perform a 4-dimensional LETKF (4D-LETKF) which assimilates asynchronous observation data at their right time within a window from hour 3 to hour 9.

The source of the observation data is from the NCEP PREPBUFR dataset that not only provides the observed values but also the observation errors associated with each observation. These observation errors will be used in our system. There are two routes of the observation processing:
 # The route 1 shown in green arrows uses the built-in observation operators that can only process conventional (non-radiance) observation data.
 # The route 2 shown in blue arrows uses the GSI as the observation operator.

A set of reference model analysis data (gray rectangle) is needed in order to provide updated values of some prognostic variables that are not able to be analyzed by the atmospheric data assimilation system, such as ozone concentration and sea surface temperature (SST). It can also be used for the verification.

= Prepare model and observation data =

We need to prepare a set of reference model data in order to update some "boundary conditions" such as SST and ozone concentration during the data assimilation cycle. Therefore, this dataset should cover the entire experiment period. It can also be used for verification. The NCEP has put their Climate Forecast System version 2 Reanalysis (CFSR) [http://nomads.ncdc.noaa.gov/data.php#cfs online] over the period of 1979–2011. They provide the model initial conditions in the GFS sig/sfc format at the [http://nomads.ncdc.noaa.gov/modeldata/cmd_LIC/ T126] and the [http://nomads.ncdc.noaa.gov/modeldata/cmd_HIC/ T382] resolutions. It is convenient to use this dataset as the reference model data for the GFS-LETKF.

There are scripts to download the CFSR (sig/sfc) data, to change the resolution, and to convert them to the gridded data in both model levels (grd) and pressure levels (grdp). In the following demonstration, everything will be run at the T62 resolution, so we are going to download the T126 data and convert them to T62.

== Download the reference model data ==

 * Go to the GFS run directory.
{{{
$ cd gfs/run
}}}
 * Edit the '{{{configure.sh}}}' file. Set '{{{$OUTDIR}}}' to an existing empty directory, and set other variables according to the [#Main_configuration_script variable description]. Pay attention to the '{{{$TMP1}}}', '{{{$EXECGLOBAL}}}', '{{{$FIXGLOBAL}}}', '{{{$ANLGFS}}}', '{{{$ANLGRD}}}', and '{{{$ANLGRDP}}}' variables that will be used in this section. Create empty directories for '{{{$ANLGFS}}}', '{{{$ANLGRD}}}', and '{{{$ANLGRDP}}}'.
{{{
$ mkdir $OUTDIR $ANLGFS $ANLGRD $ANLGRDP
}}}
 * Create a directory to store the T126 CFSR data.
{{{
$ mkdir $CFSR_T126
}}}
 * Use the '{{{get_cfsr.sh}}}' script to download the CFSR data. Note that you can run it without any argument to see the usage help.
{{{
$ ./get_cfsr.sh

[get_cfsr.sh] Download NCEP CFSR.
                 *use settings in 'configure.sh'

Usage: ./get_cfsr.sh STIME [ETIME] [GFS_DIR]

  STIME    Start time (format: YYYYMMDD)
  ETIME    End   time (format: YYYYMMDD)
           (default: same as STIME)
  GFS_DIR  Directory to save the CFSR sig/sfc files
           (default: $ANLGFS in 'configure.sh')

$ ./get_cfsr.sh 20080101 20080110 $CFSR_T126
}}}
 * Then you will see the CFSR data from January 1 to 10, 2008 are saved to the '{{{$CFSR_T126}}}' directory following the '{{{YYYYMMDDHH.sig}}}' and '{{{YYYYMMDDHH.sfc}}}' convention.

== Change the resolution of the reference model data ==

 * Use the '{{{run_chgres.sh}}}' script to change the resolution of the CFSR data from T126 to T62. The '{{{$ANLGFS}}}' below should be the same directory as the variable in '{{{configure.sh}}}'.
{{{
$ ./run_chgres.sh

[run_chgres.sh] Change the resolution of GFS initial conditions.
                *use settings in 'configure.sh'

Usage: ./run_chgres.sh STIME ETIME JCAP_NEW LONB_NEW LATB_NEW GFS_INDIR GFS_OUTDIR [FORMAT]

  STIME       Start time (format: YYYYMMDDHH)
  ETIME       End   time (format: YYYYMMDDHH)
  JCAP_NEW    New JCAP (spectral truncation)
  LONB_NEW    New LONB (number of longitudes)
  LATB_NEW    New LATB (number of latitudes)
  GFS_INDIR   Input  directory of GFS sig/sfc files
  GFS_OUTDIR  Output directory of GFS sig/sfc files
  FORMAT      Input data format
              0: Do not specify the input data format
              1: GFS/GDAS data
              2: CFSR data [- 2010-12-31 18]
              3: CFSR data [2011-01-01 00 -]
              (default: 1)

$ ./run_chgres.sh 2008010100 2008011018 62 192 94 $CFSR_T126 $ANLGFS 2
}}}
 * Then you will see the CFSR data from January 1 to 10, 2008 are converted to the T62 resolution, saved to the '{{{$ANLGFS}}}' directory.

== Convert the reference model data to gridded data ==

 * Use the '{{{mss2grd.sh}}}' script to convert the T62 CFSR sig/sfc data (in '{{{$ANLGFS}}}' directory) to the gridded data in both model levels and pressure levels (grd/grdp; in '{{{$ANLGRD}}}' and '{{{$ANLGRDP}}}' directories). Pay attention to the '{{{$ANLGFS}}}', '{{{$ANLGRD}}}', and '{{{$ANLGRDP}}}' variables in the '{{{configure.sh}}}' file.
{{{
$ ./mss2grd.sh

[mss2grd.sh] Convert NCEP sig/sfc files to GrADS grd/grdp files.
             *use settings in 'configure.sh'

Usage: ./mss2grd.sh STIME [ETIME]

  STIME  Start time (format: YYYYMMDDHH)
  ETIME  End   time (format: YYYYMMDDHH)
         (default: same as STIME)

$ ./mss2grd.sh 2008010100 2008011018
}}}
 * Then you will see the CFSR data from January 1 to 10, 2008 are converted to the gridded data following the '{{{YYYYMMDDHH.grd}}}' convention, saved to the '{{{$ANLGRD}}}' and '{{{$ANLGRDP}}}' directories.
 * You can create GrADS CTL files by the '{{{grdctl}}}' program and use the GrADS software to visualize those data.
{{{
$ ../ssio/grdctl
 
Usage: ./grdctl DSET OPTIONS T_START T_INT T_NUM GRD_TYPE
 
  GRD_TYPE: 's': Sigma-level output
            'x': Extended sigma-level output
            'p': Pressure-level output
 
FORTRAN STOP
$ ../ssio/grdctl "%y4%m2%d2%h2.grd" "template byteswapped" 00Z01Jan2008 6hr 10000 x > $ANLGRD/yyyymmddhhx.ctl
$ ../ssio/grdctl "%y4%m2%d2%h2.grd" "template byteswapped" 00Z01Jan2008 6hr 10000 p > $ANLGRDP/yyyymmddhhp.ctl
}}}

== Prepare the GFS initial conditions ==

One of the simplest ways to create an initial ensemble is using a combination of initial conditions at different times. In this way, a series of reference model data, such as the CFSR, at an unrelated time period could be used to form an initial ensemble. However, when we choose the CFSR to do so, a consistent temperature bias is observed near the tropopause which can be as large as -8 K, especially near the polar region. It is an unacceptable huge bias that can significantly degrade the LETKF data assimilation performance. For this reason, it is recommended to use the operational GFS model analyses to form the initial ensemble.

The latest one month operational GFS model analyses can be download from [http://nomads.ncep.noaa.gov/pub/data/nccf/com/gfs/prod/] . Earlier data may not be found via the public Internet. Please put these data into the '{{{$INITGFS}}}' directory configured in '{{{configure.sh}}}'. Note that these data also need to be converted to the T62 resolution before using them.

== Download the NCEP observation data ==

 * Get and compile the '{{{grabbufr}}}' utility that is used to pre-process the NCEP BUFR data. It can be downloaded from [http://ftp.emc.ncep.noaa.gov/gc_wmb/jwoollen/grabbufr/ here]. Please compile it and name the executable file as '{{{grabbufr}}}', put into the '{{{$BUFRBIN}}}' directory set in '{{{configure.sh}}}'.

 * Edit the '{{{configure.sh}}}' file. Pay attention to the '{{{$OBS}}}', '{{{$OBSNCEP}}}', and '{{{$BUFRBIN}}}' variables. Create empty directories for '{{{$OBS}}}' and '{{{$OBSNCEP}}}'.
{{{
$ mkdir $OBS $OBSNCEP
}}}
 * We are going to download the NCEP PREPBUFR data from the [http://rda.ucar.edu/datasets/ds337.0/ CISL Research Data Archive]. You need to register (free) on this website to get an account.
 * Use the '{{{get_ncepobs.sh}}}' script to download the NCEP PREPBUFR data and convert them to the LETKF observation format.
{{{
$ ./get_ncepobs.sh

[get_ncepobs.sh] Download NCEP conventional observation data.
                 *use settings in 'configure.sh'

Usage: ./get_ncepobs.sh EMAIL PASSWD STIME [ETIME] [IF_DECODE]

  EMAIL      UCAR/DSS account (register at http://rda.ucar.edu )
  PASSWD     UCAR/DSS account password
  STIME      Start time (format: YYYYMMDD)
  ETIME      End   time (format: YYYYMMDD)
             (default: same as STIME)
  IF_DECODE  Convert PREPBUFR to LETKF obs format or not
             0: No,  store only PREPBUFR format
             1: Yes, decode to LETKF obs format
             (default: Yes)

$ ./get_ncepobs.sh EMAIL PASSWD 20080101 20080110 1
}}}
 * Then you will see the NCEP PREPBUFR data from January 1 to 10, 2008 are saved in the '{{{$OBSNCEP}}}' directory, and they are also converted to the LETKF observation format saved in the '{{{$OBS}}}' directory.

= Run data assimilation cycles =

Here we demonstrate how to run the data assimilation cycle with 32 members.

== Create the initial ensemble ==

 * Check the '{{{configure.sh}}}' file, set the '{{{$MEMBER}}}' variable to 32 if it is not.
 * Edit the '{{{common/common_letkf.f90}}}' file. Set the '{{{nbv}}}' variable to 32 (default is 20).
 * Recompile all GFS-LETKF programs.
{{{
$ cd ..   (go to the 'gfs' top directory)
$ make
$ cd run
}}}
 * The '{{{init.sh}}}' script takes a series of the model analysis fields at an unrelated time period, saved in the '{{{$INITGFS}}}' directory in '{{{configure.sh}}}', to form the initial ensemble. Below we use the operational GFS model analyses from 00Z December 15, 2011 to 00Z January 15, 2012 with a 24-hour interval to form a 32-member initial ensemble. Note that the 24-hour interval is recommended in order to avoid the issue with the diurnal cycle.
{{{
$ ./init.sh

[init.sh] Prepare a initial ensemble from analyses at different time.
          *use settings in 'configure.sh'

Usage: ./init.sh STIME RTIME [RINT_H]

  STIME   Initial time of the ensemble (format: YYYYMMDDHH)
  RTIME   An arbitrary time that the first member is taken from (format: YYYYMMDDHH)
  RINT_H  Interval between each members (hour) (default: 24)

  For example, if RTIME = 1991010100, RINT_H = 24,
  then the initial ensemble are constructed by analyses at
  1991010100, 1991010200, 1991010300, ...

$ ./init.sh 2008010100 2011121500 24
}}}
 * Then you will see the output directory structure is created in the '{{{$OUTDIR}}}' directory and the initial ensemble is prepared in the '{{{$OUTDIR/anal/mmm}}}' directories.

== Run forecast/analysis cycles ==

 * Now we are going to run parallel programs. Before running parallel programs, we need to create the "machinefile" listing the computing nodes available for use. In the '{{{gfs/run/machine}}}' directory, there is a example of the machinefile. One CPU core corresponds to one line. Note: if your system has a job scheduling system, skip this two steps and see the following explanation.
{{{
$ cp machine/machinefile_4nodes machinefile
}}}
 * Use the '{{{mcycle.sh}}}' script to run multiple forecast/analysis cycles. Here we run 4 cycles (1 day) for demonstration.
{{{
$ ./mcycle.sh

[mcycle.sh] Run multiple cycles.
            *use settings in 'configure.sh'

Usage: ./mcycle.sh STIME [ETIME]

  STIME   Time of the first cycle (format: YYYYMMDDHH)
  ETIME   Time of the last  cycle (format: YYYYMMDDHH)
          (default: same as STIME)

$ ./mcycle.sh 2008010100 2008010118 &
}}}
 * If you use a cluster with a job scheduling system, you need to submit jobs via the system. Instead of doing the above two steps, you need to create a script to submit the job according to your job scheduling system. Copy the machinefile the scheduling system gives to '{{{gfs/run/machinefile}}}' before running the GFS-LETKF scripts. An example of the script for the PBS system is provided at '{{{pbs_example.sh}}}'.
 * The data assimilation cycles take a while to finish. You can monitor the progress and (possible) error messages in the '{{{gfs/run/log}}}' directory while the execution. Using 32 cores of the AMD Opteron(tm) 2378 CPU at 2.4 GHz (8 X quad-core processors), it takes roughly 15 minutes for one 6-hour forecast/analysis cycle, and 1 hour for 4 cycles.
 * Finally you will see the results in the '{{{$OUTDIR}}}' directory. You can use the GrADS software to visualize the results in the '{{{$OUTDIR/guesg}}}', '{{{$OUTDIR/analg}}}' (model levels) and the '{{{$OUTDIR/guesgp}}}', '{{{$OUTDIR/analgp}}}' (pressure levels) directories.

= Run (ensemble) forecasts and verification =

Here we demonstrate how to run the 5-day forecasts initialized from the ensemble mean analysis every cycle.

 * Use the '{{{mfcst.sh}}}' script to run multiple 5-day forecasts initialized from the 4-cycle ensemble means we previously got with the data assimilation cycles. The four cycles can be run parallelly in order to accelerate the computation. The last argument '{{{1}}}' means performing verification as well after the forecasts finish. Type the '{{{mfcst.sh}}}' command without any argument to see the usage help.
{{{
$ ./mfcst.sh

[mfcst.sh] Run multiple ensemble mean forecasts.
           *use settings in 'configure.sh'

Usage: ./mfcst.sh STIME [ETIME] [CYCLES] [IF_VERF]

  STIME    Time of the first cycle (format: YYYYMMDDHH)
  ETIME    Time of the last  cycle (format: YYYYMMDDHH)
           (default: same as STIME)
  CYCLES   Number of forecast cycles run in parallel
           (default: 1)
  IF_VERF  Run verification or not
           0: Do not run verification
           1: Run verification
           (default: 0)

$ ./mfcst.sh 2008010106 2008010200 4 1 &
}}}
 * You can monitor the progress and (possible) error messages in the '{{{gfs/run/log}}}' directory while the execution. This ensemble mean forecasts and verification takes roughly 15 minutes using the same configuration of the 32 cores.
 * You will see the results in the '{{{$OUTDIR/fcst*}}}' directory. You can use the GrADS software to visualize the results in the '{{{$OUTDIR/fcstg}}}', '{{{$OUTDIR/fcstgp}}}' (sorted according to the initial times) and the '{{{$OUTDIR/fcstv}}}', '{{{$OUTDIR/fcstvp}}}' (sorted according to the forecast hours) directories. The results of verification against (rawinsonde) observations are saved in the '{{{$OUTDIR/verfo1}}}' directory and verification against reference model analyses are saved in the '{{{$OUTDIR/verfa1}}}' directory.

= Compute the EFSO =

Here we demonstrate how to compute the ensemble forecast sensitivity to observations (EFSO) with the 6 hour evaluation time. To compute the EFSO at time t = 0, evaluated at t = 6 h, we will use the forecasts from the ensemble mean at t = 0 and t = -6 h, both valid at t = 6 h. This should be already done by the '{{{mfcst.sh}}}' step. Besides, it also needs the *ensemble* forecasts from all members at t = 0, valid at t = 6 h, which can be done by '{{{efsofcst.sh}}}' separately. In addition, we further need the observation values in the analysis at t = 0 [H(X^a^)], which need to be output from the LETKF program with a flag turned on.

 * We are going to compute the EFSO at 12Z January 1, 2008. We need to compute the LETKF analysis step again at this time if the [H(X^a^)] output flag was not turned on. Edit the '{{{gfs/letkf/letkf_obs.f90}}}' file, set both the '{{{obsgues_output}}}' and '{{{obsanal_output}}}' variables to '{{{.TRUE.}}}'.
 * Recompile all GFS-LETKF programs.
{{{
$ cd ..   (go to the 'gfs' top directory)
$ make
$ cd run
}}}
 * Edit the '{{{configure.sh}}}' file, set '{{{EFSOT}}}' to '{{{6}}}' and set '{{{$OBSOUT_DIAG}}}' to '{{{1}}}'.
 * Run the LETKF data assimilation cycle again with the [H(X^a^)] output flag turned on.
{{{
$ ./mcycle.sh 2008010106 &
}}}
 * Then the [H(X^a^)] data will be saved to '{{{$OUTDIR/obs/obsdiag/2008010112}}}'. They are in the [#Data_formats "letkfobs2" format].
 * Assuming the ensemble mean forecasts ('{{{mfcst.sh}}}') have been conducted, use the '{{{efsofcst.sh}}}' script to conduct the 6-hour ensemble forecasts.
{{{
$ ./efsofcst.sh

[efsofcst.sh] Run ensemble forecasts for EFSO computation.
              *use settings in 'configure.sh'

Usage: ./efsofcst.sh STIME [ETIME] [IF_MEAN] [CYCLES]

  STIME    Time of the first cycle (format: YYYYMMDDHH)
  ETIME    Time of the last  cycle (format: YYYYMMDDHH)
           (default: same as STIME)
  IF_MEAN  Also run the ensemble mean forecast?
           0: No
           1: Yes
           (default: 0)
  CYCLES   Number of forecast cycles run in parallel
           (default: 1)

$ ./efsofcst.sh 2008010112 2008010112 &
}}}
 * You can monitor the progress and (possible) error messages in the '{{{gfs/run/log}}}' directory.
 * Compute the EFSO at 12Z January 1, 2008 by the '{{{mefso.sh}}}' script.
{{{
$ ./mefso.sh

[mefso.sh] Run EFSO for multiple cycles.
            *use settings in 'configure.sh'

Usage: ./mefso.sh STIME [ETIME] [EFT] [LOCADV_RATE] [WMOIST]

  STIME        Time of the first cycle (format: YYYYMMDDHH)
  ETIME        Time of the last  cycle (format: YYYYMMDDHH)
               (default: same as STIME)
  EFT          Evaluation forecast time (hours)
               (default: 24)
  LOCADV_RATE  Localization advection rate relative to the phase velocity (winds)
               0: No advection
               (default: 0)
  WMOIST       Wight for the moist term in the energy norm (dimensionless)
               (default: 1)

$ ./mefso.sh 2008010112 2008010112 6 &
}}}
 * You can monitor the progress and (possible) error messages in the '{{{gfs/run/log}}}' directory, and then the EFSO results will be stored in the '{{{$OUTDIR/efso}}}' directory, also in the [#Data_formats "letkfobs2" format].